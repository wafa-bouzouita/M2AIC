# About
Recently, neural networks trained end-to-end have obtained impressive results in many problems related to natural languages (e.g. machine translation). These deep learning techniques do not rely on manual feature extraction or rule-based systems. However, behind the scenes a large part of this success is due to the development of neural architectures that are able to handle structured inputs and outputs.

In this course, we will study how build neural networks for problems related to natural languages. Specifically, we will learn how to:

* build dynamic computation graphs for sequential inputs,
* predict structures (e.g. text generation, semantic parsing),
* introduce inductive bias.

Moreover, we will develop a critical analysis of state-of-the-art NLP models:

* do they actually learn what we expect?
* how does deep for learning for NLP handle bias in training data?
