{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical classes\n",
    "#### Fait par Wafa Bouzouita et Dhiaeddine Youssfi\n",
    "\n",
    "All exercices will be in Python. It is important that you keep track of exercices and structure you code correctly (e.g. create funcions that you can re-use later)\n",
    "\n",
    "We will use Jupyter notebooks (formerly known as IPython). You can read the following courses for help:\n",
    "* Python and numpy: http://cs231n.github.io/python-numpy-tutorial/\n",
    "* Jupyter / IPython : http://cs231n.github.io/ipython-tutorial/\n",
    "\n",
    "To run this notebook:\n",
    "* create a directory somewhere on your filesystem\n",
    "* download the .ipynb from the course website: http://teaching.caio-corro.fr/2019-2020/OPT8/\n",
    "* move the .ipynb into the directory\n",
    "* from a terminal:\n",
    "    * cd /directory/path\n",
    "    * jupyter notebook\n",
    "\n",
    "Each group must send me their work by e-mail (one mail per group) before next course:\n",
    "* complete the code with comments\n",
    "* quick report on what's going one, experimental results etc.\n",
    "\n",
    "If you don't want to use the notebook, send me the python code + a PDF with plots and answers.\n",
    "\n",
    "\n",
    "# Neural network: first experiments with a linear model\n",
    "\n",
    "In this first lab exercise we will code a neural network using numpy, without a neural network library.\n",
    "Next week, the lab exercise will be to extend this program with hidden layers and activation functions.\n",
    "\n",
    "The task is digit recognition: the neural network has to predict which digit in $\\{0...9\\}$ is written in the input picture. We will use the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset, a standard benchmark in machine learning.\n",
    "\n",
    "The model is a simple linear  classifier $o = \\operatorname{softmax}(Wx + b)$ where:\n",
    "* $x$ is an input image that is represented as a column vector, each value being the \"color\" of a pixel\n",
    "* $W$ and $b$ are the parameters of the classifier\n",
    "* $\\operatorname{softmax}$ transforms the output weight (logits) into probabilities\n",
    "* $o$ is column vector that contains the probability of each category\n",
    "\n",
    "We will train this model via stochastic gradient descent by minimizing the negative log-likelihood of the data:\n",
    "$$\n",
    "    \\hat{W}, \\hat{b} = \\operatorname{argmin}_{W, b} \\sum_{x, y} - \\log p(y | x)\n",
    "$$\n",
    "Although this is a linear model, it classifies raw data without any manual feature extraction step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libs that we will use\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# To load the data we will use the script of Gaetan Marceau Caron\n",
    "# You can download it from the course webiste and move it to the same directory that contains this ipynb file\n",
    "import dataset_loader\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download mnist dataset \n",
    "if(\"mnist.pkl.gz\" not in os.listdir(\".\")):\n",
    "    !wget http://deeplearning.net/data/mnist/mnist.pkl.gz\n",
    "\n",
    "# if you have it somewhere else, you can comment the lines above\n",
    "# and overwrite the path below\n",
    "mnist_path = \"./mnist.pkl.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the 3 splits\n",
    "train_data, dev_data, test_data = dataset_loader.load_mnist(mnist_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each dataset is a list with two elemets:\n",
    "* data[0] contains images\n",
    "* data[1] contains labels\n",
    "\n",
    "Data is stored as numpy.ndarray. You can use data[0][i] to retrieve image number i and data[1][i] to retrieve its label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_data))\n",
    "print(type(train_data[0]))\n",
    "print(type(train_data[1]))\n",
    "print(type(train_data[0][0]))\n",
    "print(type(train_data[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd7aab44c18>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nO5AEYQeSIOxAEoQdSIKwA0kQdiCJ/wcymyCs2sqGbwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 900\n",
    "label = train_data[1][index]\n",
    "picture = train_data[0][index]\n",
    "print(\"label: %i\" % label)\n",
    "plt.imshow(picture.reshape(28,28), cmap='Greys')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What are the characteristics of training data? (number of samples, dimension of input, number of labels)\n",
    "\n",
    "The documentation of ndarray class is available here: https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDimDataset(data):\n",
    "    \"\"\"\n",
    "    input : our data\n",
    "    output:\n",
    "    n_training : number of samples\n",
    "    n_feature  : dimension of input\n",
    "    n_label    : number of labels\n",
    "     \n",
    "    \"\"\"\n",
    "    n_training, n_feature = train_data[0].shape\n",
    "    n_label = len(set(data[1]))\n",
    "    \n",
    "    return n_training, n_feature, n_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples  :  50000\n",
      "Dimension of input :  784\n",
      "Number of labels   :  10\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of samples  : \" ,getDimDataset(train_data)[0] )\n",
    "print(\"Dimension of input : \" ,getDimDataset(train_data)[1] )\n",
    "print(\"Number of labels   : \" ,getDimDataset(train_data)[2] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Building functions\n",
    "\n",
    "We now need to build functions that are required for the neural network.\n",
    "$$\n",
    "    o = \\operatorname{softmax}(Wx + b) \\\\\n",
    "    L(x, y) = -\\log p(y | x) = -\\log o[y]\n",
    "$$\n",
    "\n",
    "Note that in numpy, operator @ is used for matrix multiplication while * is used for element-wise multiplication.\n",
    "The documentation for linear algebra in numpy is available here: https://docs.scipy.org/doc/numpy/reference/routines.linalg.html\n",
    "\n",
    "The first operation is the affine transformation $v = Wx + b$.\n",
    "To compute the gradient, it is often convenient to write the forward pass as $v[i] = b[i] + \\sum_j W[i, j] x[j]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input:\n",
    "# - W: projection matrix\n",
    "# - b: bias\n",
    "# - x: input features\n",
    "# Output:\n",
    "# - vector\n",
    "def affine_transform(W, b, x):\n",
    "    v = W @ x + b \n",
    "    return  v\n",
    "\n",
    "# Input:\n",
    "# - W: projection matrix\n",
    "# - b: bias\n",
    "# - x: input features\n",
    "# - g: incoming gradient\n",
    "# Output:\n",
    "# - g_W: gradient wrt W\n",
    "# - g_b: gradient wrt b\n",
    "def backward_affine_transform(W, b, x, g):\n",
    "    g_W =  g.reshape(-1,1) @ np.transpose(x.reshape((-1,1)))\n",
    "    g_b =  g\n",
    "    return g_W, g_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell is a (too simple) test of affine_transform and backward_affine_transform.\n",
    "It should run without error if your implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = np.asarray([[ 0.63024213,  0.53679375, -0.92079597],\n",
    " [-0.1155045,   0.62780356, -0.67961305],\n",
    " [ 0.08465286, -0.06561815, -0.39778322],\n",
    " [ 0.8242268,   0.58907262, -0.52208052],\n",
    " [-0.43894227, -0.56993247,  0.09520727]])\n",
    "b = np.asarray([ 0.42706842,  0.69636598, -0.85611933, -0.08682553,  0.83160079])\n",
    "x = np.asarray([-0.32809223, -0.54751413,  0.81949319])\n",
    "\n",
    "o_gold = np.asarray([-0.82819732, -0.16640748, -1.17394705, -1.10761496,  1.36568213])\n",
    "g = np.asarray([-0.08938868,  0.44083873, -0.2260743,  -0.96196726, -0.53428805])\n",
    "g_W_gold = np.asarray([[ 0.02932773,  0.04894156, -0.07325341],\n",
    " [-0.14463576, -0.24136543,  0.36126434],\n",
    " [ 0.07417322,  0.12377887, -0.18526635],\n",
    " [ 0.31561399,  0.52669067, -0.78832562],\n",
    " [ 0.17529576,  0.29253025, -0.43784542]])\n",
    "g_b_gold = np.asarray([-0.08938868,  0.44083873, -0.2260743,  -0.96196726, -0.53428805])\n",
    "\n",
    "# quick test of the forward pass\n",
    "o = affine_transform(W, b, x)\n",
    "if o.shape != o_gold.shape:\n",
    "    raise RuntimeError(\"Unexpected output dimension: got %s, expected %s\" % (str(o.shape), str(o_gold.shape)))\n",
    "if not np.allclose(o, o_gold):\n",
    "    raise RuntimeError(\"Output of the affine_transform function is incorrect\")\n",
    "    \n",
    "# quick test if the backward pass\n",
    "g_W, g_b = backward_affine_transform(W, b, x, g)\n",
    "if g_W.shape != g_W_gold.shape:\n",
    "        raise RuntimeError(\"Unexpected gradient dimension for W: got %s, expected %s\" % (str(g_W.shape), str(g_W_gold.shape)))\n",
    "if g_b.shape != g_b_gold.shape:\n",
    "        raise RuntimeError(\"Unexpected gradient dimension for b: got %s, expected %s\" % (str(g_b.shape), str(g_b_gold.shape)))\n",
    "if not np.allclose(g_W, g_W_gold):\n",
    "    raise RuntimeError(\"Gradient of W is incorrect\")\n",
    "if not np.allclose(g_b, g_b_gold):\n",
    "    raise RuntimeError(\"Gradient of b is incorrect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax function:\n",
    "$$\n",
    "     o = \\operatorname{softmax}(w)\n",
    "$$\n",
    "where $w$ is a vector of logits in $\\mathbb R$ and $o$ a vector of probabilities such that:\n",
    "$$\n",
    "    o[i] = \\frac{\\exp(w[i])}{\\sum_j \\exp(w[j])}\n",
    "$$\n",
    "We do not need to implement the backward for this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input:\n",
    "# - x: vector of logits\n",
    "# Output\n",
    "# - vector of probabilities\n",
    "def softmax(x):\n",
    "    b = np.max(x)\n",
    "    y = np.exp(x-b)\n",
    "    return(y/y.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING:** is your implementation numerically stable?\n",
    "\n",
    "The $\\exp$ function results in computations that overflows (i.e. results in numbers that cannot be represented with floating point numbers).\n",
    "Therefore, it is always convenient to use the following trick to improve stability: https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Example for testing the numerical stability of softmax\n",
    "# It should return [1., 0. ,0.], not [nan, 0., 0.]\n",
    "z = [1000000,1,100]\n",
    "print(softmax(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: from the result of the cell above, what can you say about the softmax output, even when it is stable?\n",
    "\n",
    "There is zero in the softmax output, otherwise exponential function results is not supposed to give a zero value. Then, the exp-normalize trick that w applied ameliorated only the stability but the results are not perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Just too simple test for the softmax function\n",
    "x = np.asarray([0.92424884, -0.92381088, -0.74666024, -0.87705478, -0.54797015])\n",
    "y_gold = np.asarray([0.57467369, 0.09053556, 0.10808233, 0.09486917, 0.13183925])\n",
    "y = softmax(x)\n",
    "if not np.allclose(y, y_gold):\n",
    "    raise RuntimeError(\"Output of the softmax function is incorrect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we build the loss function and its gradient for training the network.\n",
    "\n",
    "The loss function is the negative log-likelihood defined as:\n",
    "$$\n",
    "    \\mathcal L(x, gold) = -\\log \\frac{\\exp(x[gold])}{\\sum_j \\exp(x[j])} = -x[gold] + \\log \\sum_j \\exp(x[j])\n",
    "$$\n",
    "This function is also called the cross-entropy loss (in Pytorch, different names are used dependending if the inputs are probabilities or raw logits).\n",
    "\n",
    "Similarly to the softmax, we have to rely on the log-sum-exp trick to stabilize the computation: https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input:\n",
    "# - x: vector of logits\n",
    "# - gold: index of the gold class\n",
    "# Output:\n",
    "# - scalare equal to -log(softmax(x)[gold])\n",
    "def nll(x, gold):\n",
    "    b = max(x)\n",
    "    y = np.sum(np.exp(x-b))\n",
    "    return(-x[gold]+b + np.log(y))\n",
    "\n",
    "# Input:\n",
    "# - x: vector of logits\n",
    "# - gold: index of the gold class\n",
    "# - gradient (scalar)\n",
    "# Output:\n",
    "# - gradient wrt x\n",
    "def backward_nll(x, gold, g):\n",
    "    vect = np.zeros(len(x))\n",
    "    vect[gold] = -1\n",
    "    g_x = vect + softmax(x)\n",
    "    return g_x * g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test\n",
    "x = np.asarray([-0.13590009, -0.83649656,  0.03130881,  0.42559402,  0.08488182])\n",
    "y_gold = 1.5695014420179738\n",
    "g_gold = np.asarray([ 0.17609875,  0.08739591, -0.79185107,  0.30875221,  0.2196042 ])\n",
    "\n",
    "y = nll(x, 2)\n",
    "g = backward_nll(x, 2, 1.)\n",
    "\n",
    "if not np.allclose(y, y_gold):\n",
    "    raise RuntimeError(\"Output is incorrect\")\n",
    "\n",
    "if g.shape != g_gold.shape:\n",
    "        raise RuntimeError(\"Unexpected gradient dimension: got %s, expected %s\" % (str(g.shape), str(g_gold.shape)))\n",
    "if not np.allclose(g, g_gold):\n",
    "    raise RuntimeError(\"Gradient is incorrect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code test the implementation of the gradient using finite-difference approximation, see: https://timvieira.github.io/blog/post/2017/04/21/how-to-test-gradient-implementations/\n",
    "\n",
    "Your implementation should pass this test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is python re-implementation of the test from the Dynet library\n",
    "# https://github.com/clab/dynet/blob/master/dynet/grad-check.cc\n",
    "\n",
    "def is_almost_equal(grad, computed_grad):\n",
    "    #print(grad, computed_grad)\n",
    "    f = abs(grad - computed_grad)\n",
    "    m = max(abs(grad), abs(computed_grad))\n",
    "\n",
    "    if f > 0.01 and m > 0.:\n",
    "        f /= m\n",
    "\n",
    "    if f > 0.01 or math.isnan(f):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def check_gradient(function, weights, true_grad, alpha = 1e-3):\n",
    "    # because input can be of any dimension,\n",
    "    # we build a view of the underlying data with the .shape(-1) method\n",
    "    # then we can access any element of the tensor as a elements of a list\n",
    "    # with a single dimension\n",
    "    weights_view = weights.reshape(-1)\n",
    "    true_grad_view = true_grad.reshape(-1)\n",
    "    for i in range(weights_view.shape[0]):\n",
    "        old = weights_view[i]\n",
    "\n",
    "        weights_view[i] = old - alpha\n",
    "        value_left = function(weights).reshape(-1)\n",
    "\n",
    "        weights_view[i] = old + alpha\n",
    "        value_right = function(weights).reshape(-1)\n",
    "\n",
    "        weights_view[i] = old\n",
    "        grad = (value_right - value_left) / (2. * alpha)\n",
    "\n",
    "        if not is_almost_equal(grad, true_grad_view[i]):\n",
    "            return False\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Test the affine transformation\n",
    "\n",
    "x = np.random.uniform(-1, 1, (5,))\n",
    "W = np.random.uniform(-1, 1, (3, 5))\n",
    "b = np.random.uniform(-1, 1, (3,))\n",
    "\n",
    "for i in range(3):\n",
    "    y = affine_transform(W, b, x)\n",
    "    g = np.zeros_like(y)\n",
    "    g[i] = 1.\n",
    "    g_W, _ = backward_affine_transform(W, b, x, g)\n",
    "    print(check_gradient(lambda W: affine_transform(W, b, x)[i], W, g_W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# test the negative likelihood loss\n",
    "\n",
    "x = np.random.uniform(-1, 1, (5,))\n",
    "\n",
    "for gold in range(5):\n",
    "    y = nll(x, gold)\n",
    "    g_y = backward_nll(x, gold, 1.)\n",
    "\n",
    "    print(check_gradient(lambda x: nll(x, gold), x, g_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Parameter initialization\n",
    "\n",
    "We are now going to build the function that will be used to initialize the parameters of the neural network before training.\n",
    "Note that for parameter initialization you must use **in-place** operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a random ndarray\n",
    "a = np.random.uniform(-1, 1, (5,))\n",
    "\n",
    "# this does not change the data of the ndarray created above!\n",
    "# it creates a new ndarray and replace the reference stored in a\n",
    "a = np.zeros((5, ))\n",
    "# this will change the underlying data of the ndarray that a points to\n",
    "a[:] = 0\n",
    "\n",
    "# similarly, this creates a new array and change the object pointed by a\n",
    "a = a + 1\n",
    "\n",
    "# while this change the underlying data of a\n",
    "a += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an affine transformation, it is common to:\n",
    "* initialize the bias to 0\n",
    "* initialize the projection matrix with Glorot initialization (also known as Xavier initialization)\n",
    "\n",
    "The formula for Glorot initialization can be found in equation 16 (page 5) of the original paper: http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def zero_init(b):\n",
    "    b[:] = 0.\n",
    "\n",
    "def glorot_init(W):\n",
    "    dim_output, dim_input = W.shape\n",
    "    W = np.random.uniform(-np.sqrt(6/dim_output+dim_input) ,np.sqrt(6/dim_output+dim_input), W.shape)\n",
    "glorot_init(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Building and training the neural network\n",
    "\n",
    "In our simple example, creating the neural network is simply instantiating the parameters $W$ and $b$.\n",
    "They must be ndarray object with the correct dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_parameters(dim_input, dim_output):\n",
    "    W = np.zeros((dim_output, dim_input))\n",
    "    b = np.zeros(dim_output)\n",
    "    \n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recent success of deep learning is (partly) due to the ability to train very big neural networks.\n",
    "However, researchers became interested in building small neural networks to improve computational efficiency and memory usage.\n",
    "Therefore, we often want to compare neural networks by their number of parameters, i.e. the size of the memory required to store the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_n_parameters(W, b):\n",
    "    dim_output, dim_input = W.shape\n",
    "    n = dim_output * dim_input + dim_output \n",
    "    print(\"W dimensions: \" + str(W.shape))\n",
    "    print(\"b dimensions: \" + str(b.shape))\n",
    "    print(\"Number of parameters: %i\" % (n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the neural network and print its number of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W dimensions: (10, 784)\n",
      "b dimensions: (10,)\n",
      "Number of parameters: 7850\n"
     ]
    }
   ],
   "source": [
    "dim_input = 784\n",
    "dim_output = 10\n",
    "W, b = create_parameters(dim_input, dim_output)\n",
    "print_n_parameters(W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update(eta, W, b, grad_w, grad_b):\n",
    "    \"\"\"\n",
    "        Update the parameters with an update rule\n",
    "        :param eta: the step-size\n",
    "        :param W: the weights\n",
    "        :param b: the bias\n",
    "        :param grad_w: the gradient w.r.t. the weights\n",
    "        :param grad_b: the gradient w.r.t. the bias\n",
    "        :type eta: float\n",
    "        :type W: ndarray\n",
    "        :type b: ndarray\n",
    "        :type grad_w: ndarray\n",
    "        :type grad_b: ndarray\n",
    "        :return: the updated parameters\n",
    "        :rtype: ndarray, ndarray\n",
    "    \"\"\"\n",
    "\n",
    "    W -= eta * grad_w\n",
    "    b -= eta * grad_b\n",
    "\n",
    "    return W,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Evaluate(W,b,data):\n",
    "    n=0\n",
    "    for j in range(len(data[0])):\n",
    "        Z = affine_transform(W,b,data[0][j])\n",
    "        out = softmax(Z)\n",
    "        y_estimate = out.argmax()\n",
    "        if y_estimate==data[1][j]:\n",
    "            n = n+1\n",
    "    return(n/len(data[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the training loop!\n",
    "\n",
    "The training loop should be structured as follows:\n",
    "* we do **epochs** over the data, i.e. one epoch is one loop over the dataset\n",
    "* at each epoch, we first loop over the data and update the network parameters with respect to the loss gradient\n",
    "* at the end of each epoch, we evaluate the network on the dev dataset\n",
    "* after all epochs are done, we evaluate our network on the test dataset and compare its performance with the performance on dev\n",
    "\n",
    "During training, it is useful to print the following information:\n",
    "* the mean loss over the epoch: it should be decreasing!\n",
    "* the accuracy on the dev set: it should be increasing!\n",
    "* the accuracy on the train set: it shoud be increasing!\n",
    "\n",
    "If you observe a decreasing loss (+increasing accuracy on test data) but decreasing accuracy on dev data, your network is overfitting!\n",
    "\n",
    "Once you have build **and tested** this a simple training loop, you should introduce the following improvements:\n",
    "* instead of evaluating on dev after each loop on the training data, you can also evaluate on dev n times per epoch\n",
    "* shuffle the data before each epoch\n",
    "* instead of memorizing the parameters of the last epoch only, you should have a copy of the parameters that produced the best value on dev data during training and evaluate on test with those instead of the parameters after the last epoch\n",
    "* learning rate decay: if you do not observe improvement on dev, you can try to reduce the step size\n",
    "\n",
    "After you conducted (successful?) experiments, you should write a report with results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math,time\n",
    "from IPython.display import clear_output\n",
    "#from aux import *\n",
    "# before training, we initialize the parameters of the network\n",
    "\n",
    "\n",
    "n_epochs = 100 # number of epochs\n",
    "step = 0.0001 # step size for gradient updates\n",
    "\n",
    "def training(step, n_epoch, verbose = False): \n",
    "    \n",
    "    # Data structures for plotting\n",
    "    g_i = []\n",
    "    m_loss=[]\n",
    "    train_acc=[]\n",
    "    dev_acc=[]\n",
    "    test_acc=[] \n",
    "    \n",
    "    bestdev = - math.inf\n",
    "\n",
    "    n_training, n_feature, n_label = getDimDataset(train_data)\n",
    "    loss = np.zeros(n_label)\n",
    "\n",
    "    cumul_time = 0.\n",
    "    \n",
    "    # create parameters \n",
    "    W, b = create_parameters(n_feature, n_label)\n",
    "    \n",
    "    # Initialize the model parameters\n",
    "    zero_init(b)\n",
    "    glorot_init(W)\n",
    "\n",
    "    for i in range(n_epoch):\n",
    "        mean_loss = 0\n",
    "        for j in range(n_training):\n",
    "\n",
    "            prev_time = time.clock()\n",
    "            ### Forward propagation\n",
    "            Z = affine_transform(W,b,train_data[0][j])\n",
    "            \n",
    "            ### Compute the loss function\n",
    "            loss = nll(Z, train_data[1][j])\n",
    "            \n",
    "            ### Compute the mean of the loss function\n",
    "            mean_loss = mean_loss + loss\n",
    "            \n",
    "            ### Compute the gradient of loss funtion wrt Z!!!!!!!!!!!\n",
    "            gout = backward_nll(Z, train_data[1][j], 1.)\n",
    "            \n",
    "            ### Compute the gradient w.r.t. parameters\n",
    "            grad_w,grad_b =  backward_affine_transform(W, b, train_data[0][j], gout)\n",
    "\n",
    "            ### Update the parameters\n",
    "            W,b = update(step, W, b, grad_w, grad_b)\n",
    "            \n",
    "            curr_time = time.clock()\n",
    "            cumul_time += curr_time - prev_time\n",
    "            \n",
    "        mean_loss /= n_training\n",
    "        \n",
    "        ### Compute the train accuracy, validation accuracy and test accuracy\n",
    "        train_accuracy = Evaluate(W,b,train_data)\n",
    "        dev_accuracy =  Evaluate(W,b,dev_data)\n",
    "        test_accuracy =  Evaluate(W,b,test_data)\n",
    "        \n",
    "        if dev_accuracy  > bestdev :\n",
    "            W_opt , b_opt = W, b\n",
    "            bestdev=dev_accuracy\n",
    "        \n",
    "    \n",
    "        result_line = str(i) + \" \" + \"the mean loss:\" + str(mean_loss) + \" the accuracy on the dev set : \" + str(dev_accuracy) + \" the accuracy on the train set :\" + str(train_accuracy) + \" the step size : \" + str(step)\n",
    "        if verbose:\n",
    "            print(result_line)\n",
    "            \n",
    "        g_i.append(i)\n",
    "        m_loss.append(mean_loss)\n",
    "        train_acc.append(train_accuracy)\n",
    "        dev_acc.append(dev_accuracy)\n",
    "        test_acc.append(test_accuracy)\n",
    "        \n",
    "    return g_i, m_loss, train_acc, dev_acc, test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 the mean loss:1.2317853847457767 the accuracy on the dev set : 0.8571 the accuracy on the train set :0.83886 the step size : 0.0001\n",
      "1 the mean loss:0.6982180834714998 the accuracy on the dev set : 0.8769 the accuracy on the train set :0.85938 the step size : 0.0001\n",
      "2 the mean loss:0.5768263782583402 the accuracy on the dev set : 0.8848 the accuracy on the train set :0.86934 the step size : 0.0001\n",
      "3 the mean loss:0.5182247621809188 the accuracy on the dev set : 0.8907 the accuracy on the train set :0.8758 the step size : 0.0001\n",
      "4 the mean loss:0.4823515463005867 the accuracy on the dev set : 0.8939 the accuracy on the train set :0.88062 the step size : 0.0001\n",
      "5 the mean loss:0.4575829091507978 the accuracy on the dev set : 0.8985 the accuracy on the train set :0.88402 the step size : 0.0001\n",
      "6 the mean loss:0.43918020853661444 the accuracy on the dev set : 0.9011 the accuracy on the train set :0.88694 the step size : 0.0001\n",
      "7 the mean loss:0.4248147031591546 the accuracy on the dev set : 0.9028 the accuracy on the train set :0.88942 the step size : 0.0001\n",
      "8 the mean loss:0.4131949499997445 the accuracy on the dev set : 0.9045 the accuracy on the train set :0.89154 the step size : 0.0001\n",
      "9 the mean loss:0.4035410588508602 the accuracy on the dev set : 0.9056 the accuracy on the train set :0.89346 the step size : 0.0001\n",
      "10 the mean loss:0.3953511819645575 the accuracy on the dev set : 0.9057 the accuracy on the train set :0.89468 the step size : 0.0001\n",
      "11 the mean loss:0.38828608746209314 the accuracy on the dev set : 0.9056 the accuracy on the train set :0.89602 the step size : 0.0001\n",
      "12 the mean loss:0.3821072652040429 the accuracy on the dev set : 0.9076 the accuracy on the train set :0.89722 the step size : 0.0001\n",
      "13 the mean loss:0.37664153836580955 the accuracy on the dev set : 0.9081 the accuracy on the train set :0.89828 the step size : 0.0001\n",
      "14 the mean loss:0.3717597501206402 the accuracy on the dev set : 0.9086 the accuracy on the train set :0.8994 the step size : 0.0001\n",
      "15 the mean loss:0.36736336295992617 the accuracy on the dev set : 0.9094 the accuracy on the train set :0.9002 the step size : 0.0001\n",
      "16 the mean loss:0.36337572222187126 the accuracy on the dev set : 0.9101 the accuracy on the train set :0.90106 the step size : 0.0001\n",
      "17 the mean loss:0.3597361814312936 the accuracy on the dev set : 0.911 the accuracy on the train set :0.90172 the step size : 0.0001\n",
      "18 the mean loss:0.3563960449314603 the accuracy on the dev set : 0.9116 the accuracy on the train set :0.90236 the step size : 0.0001\n",
      "19 the mean loss:0.35331569942213165 the accuracy on the dev set : 0.912 the accuracy on the train set :0.90302 the step size : 0.0001\n",
      "20 the mean loss:0.3504625438652768 the accuracy on the dev set : 0.9125 the accuracy on the train set :0.9037 the step size : 0.0001\n",
      "21 the mean loss:0.3478094680078015 the accuracy on the dev set : 0.9136 the accuracy on the train set :0.90436 the step size : 0.0001\n",
      "22 the mean loss:0.34533371571387317 the accuracy on the dev set : 0.9144 the accuracy on the train set :0.90488 the step size : 0.0001\n",
      "23 the mean loss:0.3430160232168574 the accuracy on the dev set : 0.9146 the accuracy on the train set :0.90558 the step size : 0.0001\n",
      "24 the mean loss:0.340839957063267 the accuracy on the dev set : 0.9147 the accuracy on the train set :0.90636 the step size : 0.0001\n",
      "25 the mean loss:0.33879139929934093 the accuracy on the dev set : 0.9148 the accuracy on the train set :0.90694 the step size : 0.0001\n",
      "26 the mean loss:0.33685814272029535 the accuracy on the dev set : 0.9151 the accuracy on the train set :0.9073 the step size : 0.0001\n",
      "27 the mean loss:0.33502956942439216 the accuracy on the dev set : 0.9154 the accuracy on the train set :0.90784 the step size : 0.0001\n",
      "28 the mean loss:0.33329639314578374 the accuracy on the dev set : 0.9159 the accuracy on the train set :0.9081 the step size : 0.0001\n",
      "29 the mean loss:0.33165045093497514 the accuracy on the dev set : 0.9159 the accuracy on the train set :0.90848 the step size : 0.0001\n",
      "30 the mean loss:0.3300845333949667 the accuracy on the dev set : 0.9162 the accuracy on the train set :0.90878 the step size : 0.0001\n",
      "31 the mean loss:0.3285922453146284 the accuracy on the dev set : 0.9163 the accuracy on the train set :0.90898 the step size : 0.0001\n",
      "32 the mean loss:0.3271678904691865 the accuracy on the dev set : 0.9167 the accuracy on the train set :0.90934 the step size : 0.0001\n",
      "33 the mean loss:0.3258063757853684 the accuracy on the dev set : 0.9168 the accuracy on the train set :0.90968 the step size : 0.0001\n",
      "34 the mean loss:0.3245031311369339 the accuracy on the dev set : 0.9169 the accuracy on the train set :0.9099 the step size : 0.0001\n",
      "35 the mean loss:0.3232540418427633 the accuracy on the dev set : 0.9168 the accuracy on the train set :0.91038 the step size : 0.0001\n",
      "36 the mean loss:0.32205539155477697 the accuracy on the dev set : 0.917 the accuracy on the train set :0.9108 the step size : 0.0001\n",
      "37 the mean loss:0.32090381369521676 the accuracy on the dev set : 0.9171 the accuracy on the train set :0.91116 the step size : 0.0001\n",
      "38 the mean loss:0.3197962499690457 the accuracy on the dev set : 0.9173 the accuracy on the train set :0.9114 the step size : 0.0001\n",
      "39 the mean loss:0.3187299147628949 the accuracy on the dev set : 0.9176 the accuracy on the train set :0.91162 the step size : 0.0001\n",
      "40 the mean loss:0.31770226446653016 the accuracy on the dev set : 0.9178 the accuracy on the train set :0.91204 the step size : 0.0001\n",
      "41 the mean loss:0.31671097093053474 the accuracy on the dev set : 0.9178 the accuracy on the train set :0.91244 the step size : 0.0001\n",
      "42 the mean loss:0.3157538984154182 the accuracy on the dev set : 0.9178 the accuracy on the train set :0.9126 the step size : 0.0001\n",
      "43 the mean loss:0.3148290835005398 the accuracy on the dev set : 0.9177 the accuracy on the train set :0.91274 the step size : 0.0001\n",
      "44 the mean loss:0.3139347175127375 the accuracy on the dev set : 0.9176 the accuracy on the train set :0.91298 the step size : 0.0001\n",
      "45 the mean loss:0.31306913110826246 the accuracy on the dev set : 0.9178 the accuracy on the train set :0.91328 the step size : 0.0001\n",
      "46 the mean loss:0.31223078070193033 the accuracy on the dev set : 0.918 the accuracy on the train set :0.91356 the step size : 0.0001\n",
      "47 the mean loss:0.3114182364864649 the accuracy on the dev set : 0.918 the accuracy on the train set :0.91386 the step size : 0.0001\n",
      "48 the mean loss:0.31063017182568126 the accuracy on the dev set : 0.9183 the accuracy on the train set :0.91418 the step size : 0.0001\n",
      "49 the mean loss:0.30986535383821645 the accuracy on the dev set : 0.9186 the accuracy on the train set :0.9144 the step size : 0.0001\n",
      "50 the mean loss:0.3091226350164304 the accuracy on the dev set : 0.9187 the accuracy on the train set :0.91456 the step size : 0.0001\n",
      "51 the mean loss:0.3084009457479989 the accuracy on the dev set : 0.9185 the accuracy on the train set :0.91476 the step size : 0.0001\n",
      "52 the mean loss:0.30769928762698673 the accuracy on the dev set : 0.9185 the accuracy on the train set :0.91514 the step size : 0.0001\n",
      "53 the mean loss:0.30701672745726 the accuracy on the dev set : 0.9187 the accuracy on the train set :0.9152 the step size : 0.0001\n",
      "54 the mean loss:0.30635239186470375 the accuracy on the dev set : 0.9188 the accuracy on the train set :0.91528 the step size : 0.0001\n",
      "55 the mean loss:0.3057054624462342 the accuracy on the dev set : 0.9189 the accuracy on the train set :0.91562 the step size : 0.0001\n",
      "56 the mean loss:0.3050751713930971 the accuracy on the dev set : 0.9188 the accuracy on the train set :0.91586 the step size : 0.0001\n",
      "57 the mean loss:0.3044607975344514 the accuracy on the dev set : 0.9192 the accuracy on the train set :0.916 the step size : 0.0001\n",
      "58 the mean loss:0.3038616627540571 the accuracy on the dev set : 0.9193 the accuracy on the train set :0.91604 the step size : 0.0001\n",
      "59 the mean loss:0.30327712873900453 the accuracy on the dev set : 0.9195 the accuracy on the train set :0.91628 the step size : 0.0001\n",
      "60 the mean loss:0.3027065940245735 the accuracy on the dev set : 0.9196 the accuracy on the train set :0.91634 the step size : 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 the mean loss:0.30214949130367436 the accuracy on the dev set : 0.9195 the accuracy on the train set :0.91644 the step size : 0.0001\n",
      "62 the mean loss:0.30160528497323447 the accuracy on the dev set : 0.9196 the accuracy on the train set :0.91654 the step size : 0.0001\n",
      "63 the mean loss:0.30107346889313363 the accuracy on the dev set : 0.9198 the accuracy on the train set :0.91668 the step size : 0.0001\n",
      "64 the mean loss:0.30055356433617175 the accuracy on the dev set : 0.9202 the accuracy on the train set :0.91686 the step size : 0.0001\n",
      "65 the mean loss:0.30004511811003465 the accuracy on the dev set : 0.9205 the accuracy on the train set :0.91712 the step size : 0.0001\n",
      "66 the mean loss:0.2995477008344187 the accuracy on the dev set : 0.9205 the accuracy on the train set :0.9173 the step size : 0.0001\n",
      "67 the mean loss:0.29906090535830004 the accuracy on the dev set : 0.9204 the accuracy on the train set :0.91738 the step size : 0.0001\n",
      "68 the mean loss:0.29858434530402306 the accuracy on the dev set : 0.9206 the accuracy on the train set :0.91756 the step size : 0.0001\n",
      "69 the mean loss:0.2981176537263778 the accuracy on the dev set : 0.9208 the accuracy on the train set :0.91776 the step size : 0.0001\n",
      "70 the mean loss:0.29766048187599836 the accuracy on the dev set : 0.9209 the accuracy on the train set :0.91784 the step size : 0.0001\n",
      "71 the mean loss:0.2972124980576328 the accuracy on the dev set : 0.921 the accuracy on the train set :0.91802 the step size : 0.0001\n",
      "72 the mean loss:0.2967733865747326 the accuracy on the dev set : 0.9212 the accuracy on the train set :0.9181 the step size : 0.0001\n",
      "73 the mean loss:0.296342846752804 the accuracy on the dev set : 0.9211 the accuracy on the train set :0.91824 the step size : 0.0001\n",
      "74 the mean loss:0.2959205920345911 the accuracy on the dev set : 0.921 the accuracy on the train set :0.9183 the step size : 0.0001\n",
      "75 the mean loss:0.2955063491409611 the accuracy on the dev set : 0.9211 the accuracy on the train set :0.91834 the step size : 0.0001\n",
      "76 the mean loss:0.2950998572918476 the accuracy on the dev set : 0.921 the accuracy on the train set :0.91842 the step size : 0.0001\n",
      "77 the mean loss:0.29470086748232427 the accuracy on the dev set : 0.9212 the accuracy on the train set :0.91848 the step size : 0.0001\n",
      "78 the mean loss:0.2943091418092086 the accuracy on the dev set : 0.9214 the accuracy on the train set :0.9186 the step size : 0.0001\n",
      "79 the mean loss:0.2939244528440223 the accuracy on the dev set : 0.9219 the accuracy on the train set :0.91872 the step size : 0.0001\n",
      "80 the mean loss:0.29354658304866443 the accuracy on the dev set : 0.9218 the accuracy on the train set :0.9188 the step size : 0.0001\n",
      "81 the mean loss:0.29317532423036596 the accuracy on the dev set : 0.9219 the accuracy on the train set :0.91888 the step size : 0.0001\n",
      "82 the mean loss:0.29281047703281066 the accuracy on the dev set : 0.9219 the accuracy on the train set :0.91894 the step size : 0.0001\n",
      "83 the mean loss:0.29245185046067484 the accuracy on the dev set : 0.922 the accuracy on the train set :0.919 the step size : 0.0001\n",
      "84 the mean loss:0.2920992614349757 the accuracy on the dev set : 0.9222 the accuracy on the train set :0.91916 the step size : 0.0001\n",
      "85 the mean loss:0.2917525343769264 the accuracy on the dev set : 0.9222 the accuracy on the train set :0.91932 the step size : 0.0001\n",
      "86 the mean loss:0.2914115008181454 the accuracy on the dev set : 0.9222 the accuracy on the train set :0.91938 the step size : 0.0001\n",
      "87 the mean loss:0.2910759990352887 the accuracy on the dev set : 0.9223 the accuracy on the train set :0.91944 the step size : 0.0001\n",
      "88 the mean loss:0.29074587370729266 the accuracy on the dev set : 0.9224 the accuracy on the train set :0.9195 the step size : 0.0001\n",
      "89 the mean loss:0.29042097559363556 the accuracy on the dev set : 0.9224 the accuracy on the train set :0.9196 the step size : 0.0001\n",
      "90 the mean loss:0.2901011612320639 the accuracy on the dev set : 0.9224 the accuracy on the train set :0.9198 the step size : 0.0001\n",
      "91 the mean loss:0.2897862926544247 the accuracy on the dev set : 0.9224 the accuracy on the train set :0.9199 the step size : 0.0001\n",
      "92 the mean loss:0.28947623711937953 the accuracy on the dev set : 0.9224 the accuracy on the train set :0.92004 the step size : 0.0001\n",
      "93 the mean loss:0.28917086686077864 the accuracy on the dev set : 0.9224 the accuracy on the train set :0.92004 the step size : 0.0001\n",
      "94 the mean loss:0.28887005885060385 the accuracy on the dev set : 0.9225 the accuracy on the train set :0.92008 the step size : 0.0001\n",
      "95 the mean loss:0.2885736945756054 the accuracy on the dev set : 0.9224 the accuracy on the train set :0.9201 the step size : 0.0001\n",
      "96 the mean loss:0.2882816598265574 the accuracy on the dev set : 0.9223 the accuracy on the train set :0.9203 the step size : 0.0001\n",
      "97 the mean loss:0.2879938444994544 the accuracy on the dev set : 0.9224 the accuracy on the train set :0.92038 the step size : 0.0001\n",
      "98 the mean loss:0.2877101424077108 the accuracy on the dev set : 0.9224 the accuracy on the train set :0.92038 the step size : 0.0001\n",
      "99 the mean loss:0.28743045110477683 the accuracy on the dev set : 0.9222 the accuracy on the train set :0.92054 the step size : 0.0001\n"
     ]
    }
   ],
   "source": [
    "g_i, m_loss, train_acc, dev_acc, test_acc = training(step, n_epochs , verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'm_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0a9e6def145e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Negative log-likelihood\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'm_loss' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(m_loss,label='mean_loss')\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Negative log-likelihood\")\n",
    "plt.legend()\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f278fd63278>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtcAAAFzCAYAAAD16yU4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEmSlJH/D7OFohzv8P54AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (12,6))\n",
    "plt.plot(train_acc,label='train_acc')\n",
    "plt.plot(dev_acc,label='valid_acc')\n",
    "plt.plot(test_acc,label='test_acc')\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Classification accuracy\")\n",
    "#plt.ylim([0.,1.])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wafe/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  \"\"\"\n",
      "/home/wafe/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize = (15,15))\n",
    "for step in [.001,0.01,0.1,1.0,10.]:\n",
    "    g_i, m_loss, train_acc, dev_acc, test_acc = training(step, 50 , verbose = False)\n",
    "    \n",
    "    plt.subplot(211)\n",
    "    plt.plot(m_loss,label='train_loss. step size = ' +str(step))\n",
    "    #plt.plot(g_i,g_valid_loss,label='valid_loss. eta = ' +str(eta))\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"Negative log-likelihood\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(212)\n",
    "    \n",
    "    plt.plot(train_acc,label='train_acc. step size = ' +str(step))\n",
    "    #plt.plot(g_i,1- g_valid_acc,label='valid_acc. eta = ' +str(eta))\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"Classification error\")\n",
    "    plt.ylim([0.,1.])\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
